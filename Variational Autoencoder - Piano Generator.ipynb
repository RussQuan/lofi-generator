{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ee6bce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIANO_GENERATOR_MODEL_PATH = 'piano_model'\n",
    "PIANO_GENERATOR_MODEL_WEIGHTS_PATH = './piano models/piano_model_vae_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e6e12619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "import pandas\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from pydub import AudioSegment\n",
    "from collections import Counter\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68171781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(path):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the 'path' directory \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    print(len(glob.glob(path + '/*.mid')))\n",
    "    for file in glob.glob(path + '/*.mid'):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "#         print('parsing %s' % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "314cc243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(path):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the 'path' directory \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    print(len(glob.glob(path + '/*.mid')))\n",
    "    for file in glob.glob(path + '/*.mid'):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3794877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_idx(idx, axis):\n",
    "    grid = np.ogrid[tuple(map(slice, idx.shape))]\n",
    "    grid.insert(axis, idx)\n",
    "    return tuple(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0944eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_initialization(a, ncols):\n",
    "    out = np.zeros(a.shape + (ncols,), dtype=int)\n",
    "    out[all_idx(a, axis=2)] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6c835613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 32\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    \n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "\n",
    "    ncols = max(max(network_input))+1\n",
    "    ncols = ncols + (ncols%4)\n",
    "    network_input = onehot_initialization(np.array(network_input), ncols)\n",
    "    return network_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e6843727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "notes = get_notes('midi_songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4eac4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(set(list(chain.from_iterable(notes))))\n",
    "network_input = prepare_sequences(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0fbf6e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1868, 32, 256)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e10d9e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "254%4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f8806",
   "metadata": {},
   "source": [
    "## Model - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbeac28",
   "metadata": {},
   "source": [
    "### Sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "15230ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9cc29",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e11fe3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 32, 256, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 128, 32)  320         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 64, 64)    18496       conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 32768)        0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 16)           524304      flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            34          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            34          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_7 (Sampling)           (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 543,188\n",
      "Trainable params: 543,188\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(32, 256, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95869850",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "eac5538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 8, 64, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DT (None, 16, 128, 64)       36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DT (None, 32, 256, 32)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DT (None, 32, 256, 1)        289       \n",
      "=================================================================\n",
      "Total params: 153,985\n",
      "Trainable params: 153,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(8 * 64 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((8, 64, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ab427",
   "metadata": {},
   "source": [
    "### VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "32bfe0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(\n",
    "                        data,\n",
    "                        reconstruction\n",
    "                    ), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7f95bf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1868, 32, 256)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "78db9627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1868, 32, 256, 1)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(network_input, -1).astype(\"float32\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "63d9d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "59/59 [==============================] - 11s 175ms/step - loss: 3233.9015 - reconstruction_loss: 1476.2596 - kl_loss: 66.1833\n",
      "Epoch 2/20\n",
      "59/59 [==============================] - 10s 171ms/step - loss: 226.9922 - reconstruction_loss: 218.3707 - kl_loss: 0.7390\n",
      "Epoch 3/20\n",
      "59/59 [==============================] - 10s 171ms/step - loss: 207.1180 - reconstruction_loss: 205.2762 - kl_loss: 0.3143\n",
      "Epoch 4/20\n",
      "59/59 [==============================] - 10s 170ms/step - loss: 201.9208 - reconstruction_loss: 201.1267 - kl_loss: 0.1213\n",
      "Epoch 5/20\n",
      "59/59 [==============================] - 10s 176ms/step - loss: 199.2603 - reconstruction_loss: 198.6637 - kl_loss: 0.1221\n",
      "Epoch 6/20\n",
      "59/59 [==============================] - 10s 177ms/step - loss: 196.6725 - reconstruction_loss: 196.0060 - kl_loss: 0.1404\n",
      "Epoch 7/20\n",
      "59/59 [==============================] - 11s 186ms/step - loss: 194.2262 - reconstruction_loss: 193.9510 - kl_loss: 0.1674\n",
      "Epoch 8/20\n",
      "59/59 [==============================] - 11s 192ms/step - loss: 192.6670 - reconstruction_loss: 192.6980 - kl_loss: 0.2352\n",
      "Epoch 9/20\n",
      "59/59 [==============================] - 11s 185ms/step - loss: 192.1818 - reconstruction_loss: 191.6225 - kl_loss: 0.4093\n",
      "Epoch 10/20\n",
      "59/59 [==============================] - 11s 184ms/step - loss: 190.8145 - reconstruction_loss: 190.0384 - kl_loss: 0.6820\n",
      "Epoch 11/20\n",
      "59/59 [==============================] - 12s 206ms/step - loss: 189.8475 - reconstruction_loss: 188.5732 - kl_loss: 1.0052\n",
      "Epoch 12/20\n",
      "59/59 [==============================] - 13s 225ms/step - loss: 188.2351 - reconstruction_loss: 187.0967 - kl_loss: 1.2962\n",
      "Epoch 13/20\n",
      "59/59 [==============================] - 13s 222ms/step - loss: 187.0376 - reconstruction_loss: 185.1883 - kl_loss: 1.5053\n",
      "Epoch 14/20\n",
      "59/59 [==============================] - 12s 209ms/step - loss: 185.0929 - reconstruction_loss: 183.6483 - kl_loss: 1.6889\n",
      "Epoch 15/20\n",
      "59/59 [==============================] - 12s 197ms/step - loss: 184.3076 - reconstruction_loss: 182.3015 - kl_loss: 1.9069\n",
      "Epoch 16/20\n",
      "59/59 [==============================] - 11s 191ms/step - loss: 183.3832 - reconstruction_loss: 180.8291 - kl_loss: 2.1238\n",
      "Epoch 17/20\n",
      "59/59 [==============================] - 12s 197ms/step - loss: 182.5769 - reconstruction_loss: 179.6177 - kl_loss: 2.3009\n",
      "Epoch 18/20\n",
      "59/59 [==============================] - 12s 202ms/step - loss: 180.2802 - reconstruction_loss: 178.6772 - kl_loss: 2.3192\n",
      "Epoch 19/20\n",
      "59/59 [==============================] - 12s 204ms/step - loss: 180.5610 - reconstruction_loss: 177.7707 - kl_loss: 2.3785\n",
      "Epoch 20/20\n",
      "59/59 [==============================] - 12s 210ms/step - loss: 179.0286 - reconstruction_loss: 176.9292 - kl_loss: 2.4820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d27e982e0>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(np.expand_dims(network_input, -1).astype(\"float32\"), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b0543029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#01: 30 epochs\n",
    "#02: 20 epochs\n",
    "vae.save_weights(PIANO_GENERATOR_MODEL_WEIGHTS_PATH + '_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ec7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6fe7d6f",
   "metadata": {},
   "source": [
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b2b746e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae1 = VAE(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d5e753b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8d5c957c70>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae1.load_weights(PIANO_GENERATOR_MODEL_WEIGHTS_PATH + '_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f31683",
   "metadata": {},
   "source": [
    "### Generate piano rolls with vae model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6f068b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, \n",
    "                output_path):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "#         offset += 0.5\n",
    "        offset += 1\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5b8c2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 32\n",
    "    network_input = []\n",
    "    \n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "\n",
    "    ncols = max(max(network_input))+1\n",
    "    ncols = ncols + (ncols%4)\n",
    "    network_input = onehot_initialization(np.array(network_input), ncols)\n",
    "\n",
    "    return network_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f2960509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, output_path):\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    #load the notes used to train the model\n",
    "    with open('data/notes', 'rb') as filepath:\n",
    "        notes = pickle.load(filepath)\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # Get all pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "\n",
    "    network_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    while True:\n",
    "        inp = np.random.normal(0,1,size=(1, latent_dim))\n",
    "        item = vae.decoder.predict(inp)[0]\n",
    "        prediction_output = []\n",
    "        for i in item:\n",
    "            predicted_note = i.flatten()\n",
    "            index = predicted_note.argmax()\n",
    "            result = int_to_note[index]\n",
    "            prediction_output.append(result)\n",
    "\n",
    "        n_unique_notes = len(Counter(prediction_output).keys())\n",
    "        if n_unique_notes > 8:\n",
    "            break\n",
    "    \n",
    "#     return prediction_output\n",
    "    create_midi(prediction_output, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "923749ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D5', 'D5', '6.9.11.2', '6.9.11.2', 'D4', 'D4', '6.9.11.2', 'D4', '11.0.4.7', '2.5.9', '2.5.9', '2.5.9', 'A2', 'A2', 'A2', 'A2', 'D3', 'D3', 'D3', 'D3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', '7.0', '4.5.9.0', '4.5.9.0', '4.5.9.0', '4.5.9.0']\n",
      "['D5', '6.9.11.2', '6.9.11.2', '6.9.11.2', '6.9.11.2', 'D4', '6.9.11.2', 'D4', '11.0.4.7', '2.5.9', 'B3', 'B3', 'A2', 'A2', 'A2', 'A2', 'D3', 'D3', 'D3', 'D3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', '7.0', '7.0', '4.5.9.0', '4.5.9.0', 'D4']\n",
      "['6.9.11.2', '6.9.11.2', '1.2.4.6.9', '8.11.1.4', '1.2.4.6.9', '1.2.4.6.9', '1.2.4.6.9', 'D3', 'D3', '2.5.9', '7.11', '7.11', '2.5.9', '2.5.9', '2.5.9', '2.5.9', 'D3', '2.5.9', 'A3', '7.10.0.3', '8.11.1.4', 'D3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'F2', '7.0', '6.10.11.1', 'G3']\n",
      "['8.11.1.4', '8.11.1.4', '8.11.1.4', '8.11.1.4', '8.11.1.4', '3.7', '3.7', 'D3', '5.9.0', '5.9.0', '5.9.0', '5.9.0', '5.9.0', '5.9.0', '5.9.0', '2.5.9', 'D3', 'D3', 'D3', 'D3', 'D3', '7.10.0.3', 'D3', 'D3', '3.6.8.10.11', '3.6.8.10.11', '3.6.8.10.11', '3.6.8.10.11', 'F2', 'F2', '3.6.8.10.11', 'B-4']\n",
      "['D5', '11.0.4.7', 'E4', '2.6.9', '2.6.9', '2.6.9', 'G3', 'D4', 'A3', 'A3', 'B3', 'B3', 'B3', 'A2', 'A3', 'A2', 'A2', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', 'B3', '7.0', 'G3', 'G3', 'A3']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate(vae, 'dsf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c2ca6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(vae, output_path='vae01_piano_05.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ccc352d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = os.system('fluidsynth -ni Touhou.sf2 vae01_piano_05.mid -F vae01_piano_05.wav -r 44100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5b411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd3a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1db751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59453dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cac8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d0671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8d600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
